We measure the predictive accuracy of our fitted models by using leave-one-out cross-validation (LOO), which is estimated using existing draws from the posterior distribution via the Pareto-smoothed
importance sampling (PSIS) method, this combined method is known as the PSIS-LOO method \cite{aki}. For each iteration of LOO the log posterior predictive density is evaluated, then iterating $N$ times the expected log pointwise predictive density can be estimated by
\begin{equation}
    \reallywidehat{\text{elppd}} = \sum_{m=1}^N \log p(y_m | \bold{y_{-m}} ) = \sum_{m=1}^N \int p(y_m | \theta) p(\theta | y) d \theta,
\end{equation}
where $\bold{y_{-m}}$ is our match data without the held-out match $m$.  
Approximating the LOO predictive density by importance sampling and applying a Pareto smoothing to the importance weights, the PSIS estimate of the LOO expected log pointwise predictive density can be written as
\begin{equation}
    \reallywidehat{\text{psis-elppd}} = \sum_{m=1}^N \log
    \left(
    \frac{\sum_{s=1}^S w_m^s p(y_m|\theta^s)}{\sum_{s=1}^S w_m^s}
    \right),
\end{equation}
with $w^s_m$ being the Pareto smoothed importance weights. The Pareto smoothing is used to regularize the importance weights, which might otherwise be very large or even infinite.
Thus, by extracting the log-likelihoods for our models we can compute the PSIS estimates of LOO and compare them to see whether we have an improved model. We use the estimates of the Pareto distributions shape parameter $k$ in order to assess the  reliability of the PSIS estimates. It is shown that PSIS estimates are reliable when $k < 0.7$ \cite{aki2}.

\bibitem{aki}
Vehtari, A., Gelman, A., Gabry, J. (2017). "Practical
Bayesian model evaluation using leave-one-out
cross-validation and WAIC". Statistics and Computing.
27(5):1413â€“1432.

\bibitem{aki2}
Vehtari, A., Simpson, D., Gelman, A., Yao, Y., Gabry,. J (2019). "Pareto Smoothed Importance Sampling" 

